{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature processing with Spark, training with XGBoost and Blazinfg algorithms and deploying as Inference Pipeline\n",
    "\n",
    "Typically a Machine Learning (ML) process consists of few steps: gathering data with various ETL jobs, pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "In many cases, when the trained model is used for processing real time or batch prediction requests, the model receives data in a format which needs to pre-processed (e.g. featurized) before it can be passed to the algorithm. In the following notebook, we will demonstrate how you can build your ML Pipeline leveraging Spark Feature Transformers and SageMaker XGBoost algorithm & after the model is trained, deploy the Pipeline (Feature Transformer and XGBoost) as an Inference Pipeline behind a single Endpoint for real-time inference and for batch inferences using Amazon SageMaker Batch Transform.\n",
    "\n",
    "In this notebook, we use EMR to run serverless Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: predict sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodologies\n",
    "The Notebook consists of a few high-level steps:\n",
    "\n",
    "* Using AWS Glue for executing the SparkML feature processing job.\n",
    "* Using SageMaker XGBoost to train on the processed dataset produced by SparkML job.\n",
    "* Building an Inference Pipeline consisting of SparkML & XGBoost models for a realtime inference endpoint.\n",
    "* Building an Inference Pipeline consisting of SparkML & XGBoost models for a single Batch Transform job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using EMR for executing the SparkML job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See notebook sparkml_serving_emr_mleap_sentiment_analysis to setup emr cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the feature processing script using SparkML\n",
    "\n",
    "The code for feature transformation using SparkML can be found in `abalone_processing.py` file written in the same directory. You can go through the code itself to see how it is using standard SparkML constructs to define the Pipeline for featurizing the data.\n",
    "\n",
    "Once the Spark ML Pipeline `fit` and `transform` is done, we are splitting our dataset into 80-20 train & validation as part of the script and uploading to S3 so that it can be used with XGBoost for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining schema of the dataset\n",
    "def get_schema_structure():\n",
    "    \n",
    "    #creating schema for dataset\n",
    "    data_schema = [\n",
    "        StructField(\"label\", IntegerType(), True),\n",
    "        StructField(\"ids\", LongType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"flag\", StringType(), True),\n",
    "        StructField(\"user\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True)\n",
    "    ]\n",
    "    return StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TEST_SIZE = 0.3\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.2\n",
    "DATA_DIR = 'data'\n",
    "VOCABULARY_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = spark.read\\\n",
    ".schema(get_schema_structure())\\\n",
    ".format('csv')\\\n",
    ".option('encoding', DATASET_ENCODING)\\\n",
    ".option('header','false')\\\n",
    ".csv('s3a://sagemaker-us-east-2-446439287457/sagemaker/twitter/data/train.csv')\\\n",
    ".select('label', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_label = df_total.withColumn(\"label\",\n",
    "              when(df_total[\"label\"] == 4, 1).otherwise(df_total[\"label\"])).select('label', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #defining a manual stop list\n",
    "def get_stop_words_list():\n",
    "            stop_words_list = ['link','google','facebook','yahoo','rt','i', 'me', 'my', 'myself', 'tag'\n",
    "                              'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                              \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his',\n",
    "                              'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    "                              'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "                              'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    "                              'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    "                              'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because',\n",
    "                              'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                              'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "                              'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                              'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "                              'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                              'only', 'own', 'same', 'so', 'than', 'too', 's', 't', 'can', 'will',\n",
    "                              'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                              'y', 'ain', 'ma', 'u', 'aren', 'ø', 'å', 'æ', 'b', 'c', 'd', 'e']\n",
    "\n",
    "            return stop_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, NGram, StopWordsRemover, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover.setStopWords(get_stop_words_list())\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
    "count_vectorizer = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\").setVocabSize(VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [ \n",
    "    tokenizer,\n",
    "    remover,\n",
    "    ngram,\n",
    "    count_vectorizer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = df_total_label.randomSplit([TRAIN_SIZE, TEST_SIZE], seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_df = model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_validation_df = model.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_line(data):\n",
    "    r = ','.join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = 'sagemaker-us-east-2-446439287457'\n",
    "s3_output_key_prefix = 'emr/sentiment/xgboost'\n",
    "s3_output_location = 's3://{}/{}/{}'.format(s3_output_bucket, s3_output_key_prefix, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_rdd = transformed_train_df.rdd.map(lambda x: (x.label, x.features))\n",
    "lines = transformed_train_rdd.map(csv_line)\n",
    "lines.coalesce(1).saveAsTextFile(s3_output_location + '/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_validation_rdd = transformed_validation_df.rdd.map(lambda x: (x.label, x.features))\n",
    "lines = transformed_validation_rdd.map(csv_line)\n",
    "lines.coalesce(1).saveAsTextFile(s3_output_location + '/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Blazing use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, NGram, StopWordsRemover, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"features\")\n",
    "remover.setStopWords(get_stop_words_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [ \n",
    "    tokenizer,\n",
    "    remover\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = df_total_label.randomSplit([TRAIN_SIZE, TEST_SIZE], seed=2)\n",
    "model = pipeline.fit(train)\n",
    "transformed_train_df = model.transform(train)\n",
    "transformed_validation_df = model.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_line(data):\n",
    "    r = ' '.join(d for d in str(data[1]))\n",
    "    return ('__label__' + str(data[0])) + \" \" + r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = 'sagemaker-us-east-2-446439287457'\n",
    "s3_output_key_prefix = 'emr/sentiment/blazing'\n",
    "s3_output_location = 's3://{}/{}/{}'.format(s3_output_bucket, s3_output_key_prefix, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing the trained Spark ML Model with [MLeap](https://github.com/combust/mleap)\n",
    "Apache Spark is best suited batch processing workloads. In order to use the Spark ML model we trained for low latency inference, we need to use the MLeap library to serialize it to an MLeap bundle and later use the [SageMaker SparkML Serving](https://github.com/aws/sagemaker-sparkml-serving-container) to perform realtime and batch inference. \n",
    "\n",
    "By using the `SerializeToBundle()` method from MLeap in the script, we are serializing the ML Pipeline into an MLeap bundle and uploading to S3 in `tar.gz` format as SageMaker expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run it if you need to clean\n",
    "import os\n",
    "os.remove('/tmp/model.zip')\n",
    "os.remove('/tmp/model.tar.gz')\n",
    "shutil.rmtree('/tmp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleSparkSerializer().serializeToBundle(model, \"jar:file:/tmp/model.zip\", transformed_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"/tmp/model.zip\") as zf:\n",
    "    zf.extractall(\"/tmp/model\")\n",
    "    \n",
    "import tarfile\n",
    "with tarfile.open(\"/tmp/model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"/tmp/model/bundle.json\", arcname='bundle.json')\n",
    "    tar.add(\"/tmp/model/root\", arcname='root')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please replace the bucket name with your bucket name where you want to upload the model\n",
    "s3 = boto3.resource('s3') \n",
    "file_name = os.path.join(\"emr/sentiment/blazing/model\", 'model.tar.gz')\n",
    "s3.Bucket('sagemaker-us-east-2-446439287457').upload_file('/tmp/model.tar.gz', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SageMaker XGBoost to train on the processed dataset produced by SparkML job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use SageMaker XGBoost algorithm to train on this dataset. We already know the S3 location\n",
    "where the preprocessed training data was uploaded as part of the Glue job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmazonSageMaker-ExecutionRole-20190904T205879\n"
     ]
    }
   ],
   "source": [
    "# Import SageMaker Python SDK to get the Session and execution_role\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "print(role[role.rfind('/') + 1:])\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next XGBoost model parameters and dataset details will be set properly\n",
    "We have parameterized this Notebook so that the same data location which was used in the PySpark script can now be passed to XGBoost Estimator as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = get_image_uri(sess.boto_region_name, 'xgboost', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = 'sagemaker-us-east-2-446439287457'\n",
    "s3_output_key_prefix = 'emr/sentiment/xgboost'\n",
    "s3_output_location = 's3://{}/{}/'.format(s3_output_bucket, s3_output_key_prefix)\n",
    "\n",
    "xgb_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.m4.xlarge',\n",
    "                                         train_volume_size = 20,\n",
    "                                         train_max_run = 3600,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location + 'xgboost_model',\n",
    "                                         sagemaker_session=sess)\n",
    "\n",
    "xgb_model.set_hyperparameters(objective = \"binary:logistic\",\n",
    "                              eta = .2,\n",
    "                              gamma = 4,\n",
    "                              max_depth = 5,\n",
    "                              num_round = 10,\n",
    "                              subsample = 0.7,\n",
    "                              silent = 0,\n",
    "                              min_child_weight = 6)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3_output_location + 'data/train', distribution='FullyReplicated', \n",
    "                        content_type='text/csv', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_output_location + 'data/validation', distribution='FullyReplicated', \n",
    "                             content_type='text/csv', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Inference Pipeline consisting of SparkML & XGBoost models for a realtime inference endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will proceed with deploying the models in SageMaker to create an Inference Pipeline. You can create an Inference Pipeline with upto five containers.\n",
    "\n",
    "Deploying a model in SageMaker requires two components:\n",
    "\n",
    "* Docker image residing in ECR.\n",
    "* Model artifacts residing in S3.\n",
    "\n",
    "**SparkML**\n",
    "\n",
    "For SparkML, Docker image for MLeap based SparkML serving is provided by SageMaker team. For more information on this, please see [SageMaker SparkML Serving](https://github.com/aws/sagemaker-sparkml-serving-container). MLeap serialized SparkML model was uploaded to S3 as part of the SparkML job we executed in AWS EMR.\n",
    "\n",
    "**XGBoost**\n",
    "\n",
    "For XGBoost, we will use the same Docker image we used for training. The model artifacts for XGBoost was uploaded as part of the training job we just ran.\n",
    "\n",
    "### Passing the schema of the payload via environment variable\n",
    "SparkML serving container needs to know the schema of the request that'll be passed to it while calling the `predict` method. In order to alleviate the pain of not having to pass the schema with every request, `sagemaker-sparkml-serving` allows you to pass it via an environment variable while creating the model definitions. This schema definition will be required in our next step for creating a model.\n",
    "\n",
    "We will see later that you can overwrite this schema on a per request basis by passing it as part of the individual request payload as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        } \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a `PipelineModel` which comprises of the SparkML and XGBoost model in the right order\n",
    "\n",
    "Next we'll create a SageMaker `PipelineModel` with SparkML and XGBoost.The `PipelineModel` will ensure that both the containers get deployed behind a single API endpoint in the correct order. The same model would later be used for Batch Transform as well to ensure that a single job is sufficient to do prediction against the Pipeline. \n",
    "\n",
    "Here, during the `Model` creation for SparkML, we will pass the schema definition that we built in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "s3_model_bucket = s3_output_bucket\n",
    "s3_model_key_prefix = 'emr/sentiment/xgboost/mleap'\n",
    "\n",
    "sparkml_data = 's3://{}/{}/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\n",
    "xgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the `PipelineModel` to an endpoint for realtime inference\n",
    "Next we will deploy the model we just created with the `deploy()` method to start an inference endpoint and we will send some requests to the endpoint to verify that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the newly created inference endpoint with a payload to transform the data\n",
    "Now we will invoke the endpoint with a valid payload that SageMaker SparkML Serving can recognize. There are three ways in which input payload can be passed to the request:\n",
    "\n",
    "* Pass it as a valid CSV string. In this case, the schema passed via the environment variable will be used to determine the schema. For CSV format, every column in the input has to be a basic datatype (e.g. int, double, string) and it can not be a Spark `Array` or `Vector`.\n",
    "\n",
    "* Pass it as a valid JSON string. In this case as well, the schema passed via the environment variable will be used to infer the schema. With JSON format, every column in the input can be a basic datatype or a Spark `Vector` or `Array` provided that the corresponding entry in the schema mentions the correct value.\n",
    "\n",
    "* Pass the request in JSON format along with the schema and the data. In this case, the schema passed in the payload will take precedence over the one passed via the environment variable (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing the payload in CSV format\n",
    "We will first see how the payload can be passed to the endpoint in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "payload = \"I love it\"\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept=CONTENT_TYPE_CSV)\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing the payload in JSON format\n",
    "We will now pass a different payload in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"data\": [\"I don't like it\"]}\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Passing the payload with both schema and the data\n",
    "Next we will pass the input payload comprising of both the schema and the data. If you notice carefully, this schema will be slightly different than what we have passed via the environment variable. The locations of `length` and `sex` column have been swapped and so the data. The server now parses the payload with this schema and works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"schema\": {\n",
    "        \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        } \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "    },\n",
    "    \"data\": [\"I like it\"]\n",
    "}\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Deleting the Endpoint\n",
    "If you do not plan to use this endpoint, then it is a good practice to delete the endpoint so that you do not incur the cost of running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = sess.boto_session\n",
    "sm_client = boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Inference Pipeline consisting of SparkML & XGBoost models for a single Batch Transform job\n",
    "SageMaker Batch Transform also supports chaining multiple containers together when deploying an Inference Pipeline and performing a single batch transform jobs to transform your data for a batch use-case similar to the real-time use-case we have seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for Batch Transform\n",
    "Batch Transform requires data in the same format described above, with one CSV or JSON being per line. For this Notebook, SageMaker team has created a sample input in CSV format which Batch Transform can process. The input is basically a similar CSV file to the training file with only difference is that it does not contain the label field.\n",
    "\n",
    "I've created a file with a few lines in S3 just to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the Transform API to create a Batch Transform job\n",
    "Next we will create a Batch Transform job using the `Transformer` class from Python SDK to create a Batch Transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = 's3://{}/{}/{}'.format(s3_output_bucket, 'emr/sentiment/xgboost/batch', 'batch_input_sentiment.csv')\n",
    "output_data_path = 's3://{}/{}/{}'.format(s3_output_bucket, 'emr/sentiment/xgboost/batch_output', timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'serial-inference-batch-' + timestamp_prefix\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    # This was the model created using PipelineModel and it contains feature processing and XGBoost\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='serial-inference-batch',\n",
    "    sagemaker_session=sess,\n",
    "    accept = CONTENT_TYPE_CSV\n",
    ")\n",
    "transformer.transform(data = input_data_path,\n",
    "                      job_name = job_name,\n",
    "                      content_type = CONTENT_TYPE_CSV, \n",
    "                      split_type = 'Line')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a `PipelineModel` which comprises of the SparkML and BlazingText model in the right order\n",
    "\n",
    "Next we'll create a SageMaker `PipelineModel` with SparkML and BlazingText.The `PipelineModel` will ensure that both the containers get deployed behind a single API endpoint in the correct order. The same model would later be used for Batch Transform as well to ensure that a single job is sufficient to do prediction against the Pipeline. \n",
    "\n",
    "Here, during the `Model` creation for SparkML, we will pass the schema definition that we built in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825641698319.dkr.ecr.us-east-2.amazonaws.com/blazingtext:latest\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'blazingtext', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_bucket = 'sagemaker-us-east-2-446439287457'\n",
    "s3_output_key_prefix = 'emr/sentiment/blazing'\n",
    "s3_output_location = 's3://{}/{}/'.format(s3_output_bucket, s3_output_key_prefix)\n",
    "\n",
    "bt_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.xlarge',\n",
    "                                         train_volume_size = 20,\n",
    "                                         train_max_run = 3600,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "\n",
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=50,\n",
    "                            min_count=100,\n",
    "                            learning_rate=0.005,\n",
    "                            vector_dim=5,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=3)\n",
    "\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3_output_location + 'data/train/part-00000', distribution='FullyReplicated', \n",
    "                        content_type='text/csv', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_output_location + 'data/validation/part-00000', distribution='FullyReplicated', \n",
    "                             content_type='text/csv', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-13 14:16:36 Starting - Starting the training job...\n",
      "2019-11-13 14:17:05 Starting - Launching requested ML instances......\n",
      "2019-11-13 14:18:00 Starting - Preparing the instances for training......\n",
      "2019-11-13 14:18:50 Downloading - Downloading input data...\n",
      "2019-11-13 14:19:29 Training - Training image download completed. Training in progress.....\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[11/13/2019 14:19:30 WARNING 140352715757376] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[11/13/2019 14:19:30 WARNING 140352715757376] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[11/13/2019 14:19:30 INFO 140352715757376] nvidia-smi took: 0.025249004364 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[11/13/2019 14:19:30 INFO 140352715757376] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[31m[11/13/2019 14:19:30 INFO 140352715757376] Processing /opt/ml/input/data/train/part-00000 . File size: 208 MB\u001b[0m\n",
      "\u001b[31m[11/13/2019 14:19:30 INFO 140352715757376] Processing /opt/ml/input/data/validation/part-00000 . File size: 89 MB\u001b[0m\n",
      "\u001b[31mRead 10M words\u001b[0m\n",
      "\u001b[31mRead 20M words\u001b[0m\n",
      "\u001b[31mRead 30M words\u001b[0m\n",
      "\u001b[31mRead 40M words\u001b[0m\n",
      "\u001b[31mRead 50M words\u001b[0m\n",
      "\u001b[31mRead 60M words\u001b[0m\n",
      "\u001b[31mRead 70M words\u001b[0m\n",
      "\u001b[31mRead 80M words\u001b[0m\n",
      "\u001b[31mRead 90M words\u001b[0m\n",
      "\u001b[31mRead 96M words\u001b[0m\n",
      "\u001b[31mNumber of words:  67\u001b[0m\n",
      "\u001b[31mLoading validation data from /opt/ml/input/data/validation/part-00000\u001b[0m\n",
      "\u001b[31mLoaded validation data.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0049  Progress: 2.00%  Million Words/sec: 11.71 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 1\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0046  Progress: 7.03%  Million Words/sec: 13.51 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.683756\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.605078\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0044  Progress: 12.03%  Million Words/sec: 13.32 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.703336\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.682646\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0041  Progress: 17.05%  Million Words/sec: 13.21 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.69721\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.690529\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 11\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.708108\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0039  Progress: 22.06%  Million Words/sec: 13.02 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 12\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.698541\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 13\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.702011\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0036  Progress: 27.08%  Million Words/sec: 13.01 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 14\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.682297\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 15\u001b[0m\n",
      "\u001b[31mUsing 4 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.701331\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 4 epochs.\u001b[0m\n",
      "\u001b[31mReached patience. Terminating training.\u001b[0m\n",
      "\u001b[31mBest epoch: 11\u001b[0m\n",
      "\u001b[31mBest validation accuracy: 0.708108\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 43.08 #####\u001b[0m\n",
      "\u001b[31mTraining finished.\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 43.08\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 111.97\n",
      "\u001b[0m\n",
      "\u001b[31m#train_accuracy: 0.7098\u001b[0m\n",
      "\u001b[31mNumber of train examples: 1120731\n",
      "\u001b[0m\n",
      "\u001b[31m#validation_accuracy: 0.7081\u001b[0m\n",
      "\u001b[31mNumber of validation examples: 479269\u001b[0m\n",
      "\n",
      "2019-11-13 14:21:52 Uploading - Uploading generated training model\n",
      "2019-11-13 14:21:52 Completed - Training job completed\n",
      "Training seconds: 182\n",
      "Billable seconds: 182\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A good way to improve the Hyperparamter tuning is to do it randomly with a script. See the example notebook hyperparameter_tuning_mxnet_gluon_cifar10_random_search for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "                        'epochs': IntegerParameter(20, 60) ,\n",
    "                        'min_count': IntegerParameter(2, 100),\n",
    "                        'vector_dim': IntegerParameter(1, 100),\n",
    "                        'word_ngrams': IntegerParameter(2, 3)\n",
    "                        }\n",
    "objective_metric_name = 'validation:accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure to import the relevant objects used to construct the tuner\n",
    "tuner = HyperparameterTuner(bt_model,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=60,\n",
    "                            max_parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': train_data, 'validation': validation_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_info = sess.sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName='blazingtext-191113-1321')\n",
    "\n",
    "# We begin by asking SageMaker to describe for us the results of the best training job. The data\n",
    "# structure returned contains a lot more information than we currently need, try checking it out\n",
    "# yourself in more detail.\n",
    "best_training_job_name = tuning_job_info['BestTrainingJob']['TrainingJobName']\n",
    "training_job_info = sess.sagemaker_client.describe_training_job(TrainingJobName=best_training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'AlgorithmSpecification': {u'MetricDefinitions': [{u'Name': u'train:accuracy',\n",
       "    u'Regex': u'#train_accuracy: (\\\\S+)'},\n",
       "   {u'Name': u'train:mean_rho', u'Regex': u'#mean_rho: (\\\\S+)'},\n",
       "   {u'Name': u'validation:accuracy',\n",
       "    u'Regex': u'#validation_accuracy: (\\\\S+)'}],\n",
       "  u'TrainingImage': u'825641698319.dkr.ecr.us-east-2.amazonaws.com/blazingtext:latest',\n",
       "  u'TrainingInputMode': u'File'},\n",
       " u'BillableTimeInSeconds': 226,\n",
       " u'CreationTime': datetime.datetime(2019, 11, 13, 13, 57, 21, 273000, tzinfo=tzlocal()),\n",
       " u'EnableInterContainerTrafficEncryption': False,\n",
       " u'EnableManagedSpotTraining': False,\n",
       " u'EnableNetworkIsolation': False,\n",
       " u'FinalMetricDataList': [{u'MetricName': u'train:accuracy',\n",
       "   u'Timestamp': datetime.datetime(1970, 1, 19, 5, 7, 33, 767000, tzinfo=tzlocal()),\n",
       "   u'Value': 0.7631000280380249},\n",
       "  {u'MetricName': u'validation:accuracy',\n",
       "   u'Timestamp': datetime.datetime(1970, 1, 19, 5, 7, 33, 767000, tzinfo=tzlocal()),\n",
       "   u'Value': 0.7613000273704529}],\n",
       " u'HyperParameters': {u'_tuning_objective_metric': u'validation:accuracy',\n",
       "  u'early_stopping': u'True',\n",
       "  u'epochs': u'23',\n",
       "  u'learning_rate': u'0.005',\n",
       "  u'min_count': u'100',\n",
       "  u'min_epochs': u'5',\n",
       "  u'mode': u'supervised',\n",
       "  u'patience': u'4',\n",
       "  u'vector_dim': u'4',\n",
       "  u'word_ngrams': u'3'},\n",
       " u'InputDataConfig': [{u'ChannelName': u'train',\n",
       "   u'CompressionType': u'None',\n",
       "   u'ContentType': u'text/csv',\n",
       "   u'DataSource': {u'S3DataSource': {u'S3DataDistributionType': u'FullyReplicated',\n",
       "     u'S3DataType': u'S3Prefix',\n",
       "     u'S3Uri': u's3://sagemaker-us-east-2-446439287457/emr/sentiment/blazing/data/train/part-00000'}},\n",
       "   u'RecordWrapperType': u'None'},\n",
       "  {u'ChannelName': u'validation',\n",
       "   u'CompressionType': u'None',\n",
       "   u'ContentType': u'text/csv',\n",
       "   u'DataSource': {u'S3DataSource': {u'S3DataDistributionType': u'FullyReplicated',\n",
       "     u'S3DataType': u'S3Prefix',\n",
       "     u'S3Uri': u's3://sagemaker-us-east-2-446439287457/emr/sentiment/blazing/data/validation/part-00000'}},\n",
       "   u'RecordWrapperType': u'None'}],\n",
       " u'LastModifiedTime': datetime.datetime(2019, 11, 13, 14, 2, 58, 307000, tzinfo=tzlocal()),\n",
       " u'ModelArtifacts': {u'S3ModelArtifacts': u's3://sagemaker-us-east-2-446439287457/emr/sentiment/blazing/blazingtext-191113-1321-019-a73e0f0f/output/model.tar.gz'},\n",
       " u'OutputDataConfig': {u'KmsKeyId': u'',\n",
       "  u'S3OutputPath': u's3://sagemaker-us-east-2-446439287457/emr/sentiment/blazing/'},\n",
       " u'ResourceConfig': {u'InstanceCount': 1,\n",
       "  u'InstanceType': u'ml.c4.xlarge',\n",
       "  u'VolumeSizeInGB': 20},\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '3432',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Wed, 13 Nov 2019 14:14:32 GMT',\n",
       "   'x-amzn-requestid': '41475738-9943-4cb8-9349-8fa31d440287'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': '41475738-9943-4cb8-9349-8fa31d440287',\n",
       "  'RetryAttempts': 0},\n",
       " u'RoleArn': u'arn:aws:iam::446439287457:role/service-role/AmazonSageMaker-ExecutionRole-20190904T205879',\n",
       " u'SecondaryStatus': u'Completed',\n",
       " u'SecondaryStatusTransitions': [{u'EndTime': datetime.datetime(2019, 11, 13, 13, 59, 12, 916000, tzinfo=tzlocal()),\n",
       "   u'StartTime': datetime.datetime(2019, 11, 13, 13, 57, 21, 273000, tzinfo=tzlocal()),\n",
       "   u'Status': u'Starting',\n",
       "   u'StatusMessage': u'Preparing the instances for training'},\n",
       "  {u'EndTime': datetime.datetime(2019, 11, 13, 13, 59, 37, 650000, tzinfo=tzlocal()),\n",
       "   u'StartTime': datetime.datetime(2019, 11, 13, 13, 59, 12, 916000, tzinfo=tzlocal()),\n",
       "   u'Status': u'Downloading',\n",
       "   u'StatusMessage': u'Downloading input data'},\n",
       "  {u'EndTime': datetime.datetime(2019, 11, 13, 14, 2, 51, 912000, tzinfo=tzlocal()),\n",
       "   u'StartTime': datetime.datetime(2019, 11, 13, 13, 59, 37, 650000, tzinfo=tzlocal()),\n",
       "   u'Status': u'Training',\n",
       "   u'StatusMessage': u'Training image download completed. Training in progress.'},\n",
       "  {u'EndTime': datetime.datetime(2019, 11, 13, 14, 2, 58, 307000, tzinfo=tzlocal()),\n",
       "   u'StartTime': datetime.datetime(2019, 11, 13, 14, 2, 51, 912000, tzinfo=tzlocal()),\n",
       "   u'Status': u'Uploading',\n",
       "   u'StatusMessage': u'Uploading generated training model'},\n",
       "  {u'EndTime': datetime.datetime(2019, 11, 13, 14, 2, 58, 307000, tzinfo=tzlocal()),\n",
       "   u'StartTime': datetime.datetime(2019, 11, 13, 14, 2, 58, 307000, tzinfo=tzlocal()),\n",
       "   u'Status': u'Completed',\n",
       "   u'StatusMessage': u'Training job completed'}],\n",
       " u'StoppingCondition': {u'MaxRuntimeInSeconds': 3600},\n",
       " u'TrainingEndTime': datetime.datetime(2019, 11, 13, 14, 2, 58, 307000, tzinfo=tzlocal()),\n",
       " u'TrainingJobArn': u'arn:aws:sagemaker:us-east-2:446439287457:training-job/blazingtext-191113-1321-019-a73e0f0f',\n",
       " u'TrainingJobName': u'blazingtext-191113-1321-019-a73e0f0f',\n",
       " u'TrainingJobStatus': u'Completed',\n",
       " u'TrainingStartTime': datetime.datetime(2019, 11, 13, 13, 59, 12, 916000, tzinfo=tzlocal()),\n",
       " u'TrainingTimeInSeconds': 226,\n",
       " u'TuningJobArn': u'arn:aws:sagemaker:us-east-2:446439287457:hyper-parameter-tuning-job/blazingtext-191113-1321'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_job_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create end point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"type\": \"string\", \"name\": \"text\"}], \"output\": {\"type\": \"string\", \"name\": \"features\", \"struct\": \"array\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"string\",\n",
    "            \"struct\": \"array\"\n",
    "        }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "sparkml_data = 's3://{}/{}/{}'.format(s3_output_bucket, s3_output_key_prefix + '/model', 'model.tar.gz')\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data,\n",
    "                             env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json, \n",
    "                                  'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': \"application/jsonlines;data=text\"})\n",
    "bt_model = Model(model_data=bt_model.model_data, image=training_image)\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, bt_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"label\": [\"__label__0\"], \"prob\": [0.9649649262428284]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "payload = \"I don't love it.\"\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept='application/jsonlines')\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"label\": [\"__label__0\"], \"prob\": [0.873008668422699]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\"data\": [\"He likes potatoes\"]}\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete endpoint\n",
    "sm_client = boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = 's3://{}/{}/{}'.format(s3_output_bucket, 'emr/sentiment/xgboost/batch', 'batch_input_sentiment.csv')\n",
    "output_data_path = 's3://{}/{}/{}'.format(s3_output_bucket, 'emr/sentiment/blazing/batch_output', timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................\u001b[31m  .   ____          _            __ _ _\n",
      " /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\u001b[0m\n",
      "\u001b[32m  .   ____          _            __ _ _\n",
      " /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\u001b[0m\n",
      "\u001b[31m( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n",
      " \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n",
      "  '  |____| .__|_| |_|_| |_\\__, | / / / /\n",
      " =========|_|==============|___/=/_/_/_/\n",
      " :: Spring Boot ::                  (v2.2)\n",
      "\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:07.681  INFO 8 --- [           main] com.amazonaws.sagemaker.App              : Starting App v2.2 on 785f6dad92c3 with PID 8 (/usr/local/lib/sparkml-serving-2.2.jar started by root in /sagemaker-sparkml-model-server)\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:07.685  INFO 8 --- [           main] com.amazonaws.sagemaker.App              : No active profile set, falling back to default profiles: default\u001b[0m\n",
      "\u001b[32m( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n",
      " \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n",
      "  '  |____| .__|_| |_|_| |_\\__, | / / / /\n",
      " =========|_|==============|___/=/_/_/_/\n",
      " :: Spring Boot ::                  (v2.2)\n",
      "\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:07.681  INFO 8 --- [           main] com.amazonaws.sagemaker.App              : Starting App v2.2 on 785f6dad92c3 with PID 8 (/usr/local/lib/sparkml-serving-2.2.jar started by root in /sagemaker-sparkml-model-server)\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:07.685  INFO 8 --- [           main] com.amazonaws.sagemaker.App              : No active profile set, falling back to default profiles: default\u001b[0m\n",
      "\u001b[33mArguments: serve\u001b[0m\n",
      "\u001b[33m[11/13/2019 14:35:07 INFO 140313684346688] Finding and loading model\u001b[0m\n",
      "\u001b[33m[11/13/2019 14:35:07 INFO 140313684346688] Trying to load model from /opt/ml/model/model.bin\u001b[0m\n",
      "\u001b[33m[11/13/2019 14:35:08 INFO 140313684346688] Number of server workers: 4\u001b[0m\n",
      "\u001b[33m[2019-11-13 14:35:08 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[33m[2019-11-13 14:35:08 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[33m[2019-11-13 14:35:08 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[33m[2019-11-13 14:35:08 +0000] [79] [INFO] Booting worker with pid: 79\u001b[0m\n",
      "\u001b[33m[2019-11-13 14:35:08 +0000] [80] [INFO] Booting worker with pid: 80\u001b[0m\n",
      "\u001b[33m[2019-11-13 14:35:08 +0000] [81] [INFO] Booting worker with pid: 81\u001b[0m\n",
      "\u001b[33m[2019-11-13 14:35:08 +0000] [82] [INFO] Booting worker with pid: 82\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.195  INFO 8 --- [           main] org.eclipse.jetty.util.log               : Logging initialized @4046ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.303  INFO 8 --- [           main] o.s.b.w.e.j.JettyServletWebServerFactory : Server initialized with port: 8080\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.309  INFO 8 --- [           main] org.eclipse.jetty.server.Server          : jetty-9.4.z-SNAPSHOT; built: 2018-08-30T13:59:14.071Z; git: 27208684755d94a92186989f695db2d7b21ebc51; jvm 1.8.0_181-8u181-b13-2~deb9u1-b13\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.495  INFO 8 --- [           main] org.eclipse.jetty.server.session         : DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.496  INFO 8 --- [           main] org.eclipse.jetty.server.session         : No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.497  INFO 8 --- [           main] org.eclipse.jetty.server.session         : node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.521  INFO 8 --- [           main] o.e.j.s.h.ContextHandler.application     : Initializing Spring embedded WebApplicationContext\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.522  INFO 8 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2553 ms\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.610  INFO 8 --- [           main] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.617  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.618  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.618  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'formContentFilter' to: [/*]\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.619  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.625  INFO 8 --- [           main] o.e.jetty.server.handler.ContextHandler  : Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@72c927f1{application,/,[file:///tmp/jetty-docbase.4697952008284415010.8080/],AVAILABLE}\u001b[0m\n",
      "\u001b[31m2019-11-13 14:35:10.626  INFO 8 --- [           main] org.eclipse.jetty.server.Server          : Started @4487ms\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.195  INFO 8 --- [           main] org.eclipse.jetty.util.log               : Logging initialized @4046ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.303  INFO 8 --- [           main] o.s.b.w.e.j.JettyServletWebServerFactory : Server initialized with port: 8080\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.309  INFO 8 --- [           main] org.eclipse.jetty.server.Server          : jetty-9.4.z-SNAPSHOT; built: 2018-08-30T13:59:14.071Z; git: 27208684755d94a92186989f695db2d7b21ebc51; jvm 1.8.0_181-8u181-b13-2~deb9u1-b13\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.495  INFO 8 --- [           main] org.eclipse.jetty.server.session         : DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.496  INFO 8 --- [           main] org.eclipse.jetty.server.session         : No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.497  INFO 8 --- [           main] org.eclipse.jetty.server.session         : node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.521  INFO 8 --- [           main] o.e.j.s.h.ContextHandler.application     : Initializing Spring embedded WebApplicationContext\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.522  INFO 8 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2553 ms\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.610  INFO 8 --- [           main] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.617  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.618  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.618  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'formContentFilter' to: [/*]\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.619  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.625  INFO 8 --- [           main] o.e.jetty.server.handler.ContextHandler  : Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@72c927f1{application,/,[file:///tmp/jetty-docbase.4697952008284415010.8080/],AVAILABLE}\u001b[0m\n",
      "\u001b[32m2019-11-13 14:35:10.626  INFO 8 --- [           main] org.eclipse.jetty.server.Server          : Started @4487ms\u001b[0m\n",
      "\n",
      "\u001b[34m2019-11-13T14:35:28.358:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=5, BatchStrategy=SINGLE_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transformer = sagemaker.transformer.Transformer(\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='serial-inference-batch',\n",
    "    sagemaker_session=sess,\n",
    "    accept = CONTENT_TYPE_CSV\n",
    ")\n",
    "transformer.transform(data = input_data_path, \n",
    "                        content_type = CONTENT_TYPE_CSV, \n",
    "                        split_type = 'Line')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference pipeline didn't work very well with xgBoost. \n",
    "\n",
    "All results are the same for the Xgboost model and it is annoying we have to write the csv to to the inference pipeline. It would be much more pratical if the pipeline managed it as it manages after trainning.\n",
    "\n",
    "Convert a sparse vector to a column in csv makes the operation trick since the file is huge. I tried to write only 10 000 results and I tested but that's not the best solution.\n",
    "\n",
    "With Blazing it works at least but I don't trust at the results at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
