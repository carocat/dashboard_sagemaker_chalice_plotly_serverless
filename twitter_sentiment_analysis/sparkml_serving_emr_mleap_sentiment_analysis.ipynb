{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an ML Model using Apache Spark in EMR and deploy in SageMaker\n",
    "In this notebook, we will see how you can train your Machine Learning (ML) model using Apache Spark and then take the trained model artifacts to create an endpoint in SageMaker for online inference. Apache Spark is one of the most popular big-data analytics platforms & it also comes with an ML library with a wide variety of feature transformers and algorithms that one can use to build an ML model. \n",
    "\n",
    "Apache Spark is designed for offline batch processing workload and is not best suited for low latency online prediction. In order to mitigate that, we will use [MLeap](https://github.com/combust/mleap) library. MLeap provides an easy-to-use Spark ML Pipeline serialization format & execution engine for low latency prediction use-cases. Once the ML model is trained using Apache Spark in EMR, we will serialize it with `MLeap` and upload to S3 as part of the Spark job so that it can be used in SageMaker in inference.\n",
    "\n",
    "After the model training is completed, we will use SageMaker **Inference** to perform predictions against this model. The underlying Docker image that we will use in inference is provided by [sagemaker-sparkml-serving](https://github.com/aws/sagemaker-sparkml-serving-container). It is a Spring based HTTP web server written following SageMaker container specifications and its operations are powered by `MLeap` execution engine. \n",
    "\n",
    "In the first segment of the notebook, we will work with `Sparkmagic (PySpark)` kernel while performing operations on the EMR cluster and in the second segment, we need to switch to `conda_python2` kernel to invoke SageMaker APIs using `sagemaker-python-sdk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup an EMR cluster and connect a SageMaker notebook to the cluster\n",
    "In order to perform the steps mentioned in this notebook, you will need to have an EMR cluster running and make sure that the notebook can connect to the master node of the cluster. \n",
    "\n",
    "**The guide in the next paragraph does not include that you have to add the ec2 key par to the cluster in order to be able to connect via ssh. See the guide https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair how you can create one.**\n",
    "\n",
    "**At this point, `sagemaker-sparkml-serving` only supports models trained with Spark version 2.2 for performing inference. Hence, please create an EMR cluster with Spark 2.2.0 or Spark 2.2.1 if you want to use your Spark ML model for online inference or batch transform.**\n",
    "\n",
    "Please follow the guide here on how to setup an EMR cluster and connect it to a notebook.\n",
    "https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/ .\n",
    "\n",
    "\n",
    "This notebook is written in Python2, but you should be able to use Python3 with minimal changes in the instruction here. Python2 or 3 has no impact on the model serialization or inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install additional Python dependencies and JARs in the EMR cluster\n",
    "In order to serialize a Spark model with `MLeap` and upload to S3, we will need some additional Python dependencies and JAR present in the EMR cluster. Also, you need to setup your cluster with proper aws configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conect to the cluster\n",
    "\n",
    "1. download emrkey\n",
    "\n",
    "2. add permission:\n",
    "\n",
    "`chmod 400 emr.pem`\n",
    "\n",
    "3. check key (this step is not mandatory): \n",
    "\n",
    "`ssh-keygen -y -f emr.pem`\n",
    "\n",
    "4. connect to the cluster using public DNS:\n",
    "\n",
    "`ssh hadoop@ec2<your-cluster-public-dns>.amazonaws.com -i emr.pem`\n",
    "\n",
    "**if you get an error related to port 22, you have to add it editing the inbound to the maste node as you made for the port 8998 but instead of using TCP use ssh and chose any user.**\n",
    "\n",
    "you should see something like that after you connect:\n",
    "\n",
    "<img src='notebook_ims/emr_screen.png' width=40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python dependencies and download mleap jar\n",
    "After you have placed the JAR in the right location, please download a couple of necessary dependencies from PyPI. You have to download `boto3` and `mleap`.\n",
    "You can run the below commands to download the dependencies from PyPI:\n",
    "\n",
    "`sudo python /usr/lib/python2.7/dist-packages/easy_install.py pip`\n",
    "\n",
    "You need to have the MLeap JAR in the classpath to be successfully able to use it during model serialization. Please download the JAR (it is an assembly/fat JAR) from the following link using `wget`:\n",
    "\n",
    "`sudo wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar /usr/lib/spark/jars`\n",
    "\n",
    "For the next step only the scikit-learn package is mandatory. If you try to install mleap without installing scikit-learn you will probably get an error which scikit-learn asks for python 3 version. I installed the other packages for pratical reasons.\n",
    "\n",
    "`sudo /usr/local/bin/pip install paramiko nltk scipy scikit-learn pandas`\n",
    "\n",
    "So you have to install boto3 and mleap\n",
    "\n",
    "`sudo pip install boto3`\n",
    "\n",
    "`sudo pip install mleap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking that the Spark connection is set up properly\n",
    "Following the blog mentioned above, we test that the Spark connection setup is done properly by invoking `%%info` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing PySpark dependencies\n",
    "Next we will import all the necessary dependencies that will be needed to execute the following cells on our Spark cluster. Please note that we are also importing the `boto3` and `mleap` modules here. \n",
    "\n",
    "You need to ensure that the import cell runs without any error to verify that you have installed the dependencies from PyPI properly. Also, this cell will provide you with a valid `SparkSession` named as `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TEST_SIZE = 0.3\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.2\n",
    "VOCABULARY_SIZE = 5000\n",
    "DATA_DIR = 'data'\n",
    "FEATURES_NUMBER = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#defining schema of the dataset\n",
    "def get_schema_structure():\n",
    "    \n",
    "    #creating schema for dataset\n",
    "    data_schema = [\n",
    "        StructField(\"label\", IntegerType(), True),\n",
    "        StructField(\"ids\", LongType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"flag\", StringType(), True),\n",
    "        StructField(\"user\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True)\n",
    "    ]\n",
    "    return StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read\\\n",
    ".schema(get_schema_structure())\\\n",
    ".format('csv')\\\n",
    ".option('encoding', DATASET_ENCODING)\\\n",
    ".option('header','false')\\\n",
    ".csv('s3a://sagemaker-us-east-2-446439287457/sagemaker/twitter/data/train.csv')\\\n",
    ".select('label', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pre_split_df = df.withColumn(\"label\",\n",
    "              when(df[\"label\"] == 4, 1).otherwise(df[\"label\"])).select('label', 'text')\n",
    "train, test = pre_split_df.randomSplit([TRAIN_SIZE, TEST_SIZE], seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning task: Predict sentimentfrom review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available from [Sentiment140 dataset with 1.6 million tweets](https://www.kaggle.com/kazanova/sentiment140). This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment . \n",
    "The target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive).\n",
    "\n",
    "We'll use SparkML to pre-process the dataset (apply one or more feature transformers) and train it with the [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) algorithm from SparkML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or read data directly from S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please replace the bucket name with your bucket-name and the file-name/key with your file-name/key\n",
    "df_train = spark.read.parquet('s3a://sagemaker-us-east-2-446439287457/sagemaker/twitter/data/processed/train.parquet')\n",
    "df_test = spark.read.parquet('s3a://sagemaker-us-east-2-446439287457/sagemaker/twitter/data/processed/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- data_prep: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)"
     ]
    }
   ],
   "source": [
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                text|           data_prep|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|   uploading pict...|[upload, pictur, ...|\n",
      "|    1|   we break dance...|[we, break, danc,...|\n",
      "|    1| &quot;I'm a free...|[&quotim, a, free...|\n",
      "|    1| - Iowa No. 2 in ...|[-, iowa, no, 2, ...|\n",
      "|    1| Ahhh, what a way...|[ahhh, what, a, w...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_train.where(col('label') == 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature transformers\n",
    "The dataset has one categorical column - `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, NGram, StopWordsRemover, Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover.setStopWords(get_stop_words_list())\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
    "count_vectorizer = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\").setVocabSize(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Logistic Regression model and perform training\n",
    "After the data is preprocessed, we define a `LogisticRegression`, define our `Pipeline` comprising of both feature transformation and training stages and train the Pipeline calling `.fit()`.\n",
    "The parameters have been defined in the notebook LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = LogisticRegression(regParam = 0.001, maxIter = 10, elasticNetParam = 0.3)\n",
    "\n",
    "pipeline = Pipeline(stages = [ \n",
    "    tokenizer,\n",
    "    remover,\n",
    "    ngram,\n",
    "    count_vectorizer,\n",
    "    lr    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformed_train_df = model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformed_test_df = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained `Model` to transform train and validation dataset\n",
    "Next we will use this trained `Model` to convert our training and validation dataset to see some sample output and also measure the performance scores.The `Model` will apply the feature transformers on the data before passing it to the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "transformed_test_df.select('prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on train and validation dataset\n",
    "Using Spark's `RegressionEvaluator`, we can calculate the `areaUnderROC` on our train and validation dataset to evaluate its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train areaUnderROC = 0.789047\n",
      "Validation areaUnderROC = 0.774873"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "train_eva = evaluator.evaluate(transformed_train_df)\n",
    "validation_eva = evaluator.evaluate(transformed_test_df)\n",
    "print(\"Train areaUnderROC = %g\" % train_eva)\n",
    "print(\"Validation areaUnderROC = %g\" % validation_eva)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `MLeap` to serialize the model\n",
    "By calling the `serializeToBundle` method from the `MLeap` library, we can store the `Model` in a specific serialization format that can be later used for inference by `sagemaker-sparkml-serving`. \n",
    "\n",
    "**If this step fails with an error - `JavaPackage is not callable`, it means you have not setup the MLeap JAR in the classpath properly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SimpleSparkSerializer().serializeToBundle(model, \"jar:file:/tmp/model.zip\", transformed_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the model to `tar.gz` format\n",
    "SageMaker expects any model format to be present in `tar.gz` format, but MLeap produces the model `zip` format. In the next cell, we unzip the model artifacts and store it in `tar.gz` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"/tmp/model.zip\") as zf:\n",
    "    zf.extractall(\"/tmp/model\")\n",
    "    \n",
    "import tarfile\n",
    "with tarfile.open(\"/tmp/model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"/tmp/model/bundle.json\", arcname='bundle.json')\n",
    "    tar.add(\"/tmp/model/root\", arcname='root')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the trained model artifacts to S3\n",
    "At the end, we need to upload the trained and serialized model artifacts to S3 so that it can be used for inference in SageMaker. \n",
    "\n",
    "Please note down the S3 location to where you are uploading your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please replace the bucket name with your bucket name where you want to upload the model\n",
    "# Please replace the bucket name with your bucket name where you want to upload the model\n",
    "s3 = boto3.resource('s3') \n",
    "file_name = os.path.join(\"emr/sentiment/lr/mleap\", 'model.tar.gz')\n",
    "s3.Bucket('sagemaker-us-east-2-446439287457').upload_file('/tmp/model.tar.gz', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete model artifacts from local disk (optional)\n",
    "If you are training multiple ML models on the same host and using the same location to save the `MLeap` serialized model, then you need to delete the model on the local disk to prevent `MLeap` library failing with an error - `file already exists`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('/tmp/model.zip')\n",
    "os.remove('/tmp/model.tar.gz')\n",
    "shutil.rmtree('/tmp/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting the model in SageMaker\n",
    "Now the second phase of this Notebook begins, where we will host this model in SageMaker and perform predictions against it. \n",
    "\n",
    "**For this, please change your kernel to `conda_python2`.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosting a model in SageMaker requires two components\n",
    "\n",
    "* A Docker image residing in ECR.\n",
    "* a trained Model residing in S3.\n",
    "\n",
    "For SparkML, Docker image for MLeap based SparkML serving has already been prepared and uploaded to ECR by SageMaker team which anyone can use for hosting. For more information on this, please see [SageMaker SparkML Serving](https://github.com/aws/sagemaker-sparkml-serving-container/). \n",
    "\n",
    "MLeap serialized model was uploaded to S3 as part of the Spark job we executed in EMR in the previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the endpoint for prediction\n",
    "Next we'll create the SageMaker endpoint which will be used for performing online prediction. \n",
    "\n",
    "For this, we have to create an instance of `SparkMLModel` from `sagemaker-python-sdk` which will take the location of the model artifacts that we uploaded to S3 as part of the EMR job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing the schema of the payload via environment variable\n",
    "SparkML server also needs to know the payload of the request that'll be passed to it while calling the `predict` method. In order to alleviate the pain of not having to pass the schema with every request, `sagemaker-sparkml-serving` lets you to pass it via an environment variable while creating the model definitions. \n",
    "\n",
    "We'd see later that you can overwrite this schema on a per request basis by passing it as part of the individual request payload as well.\n",
    "\n",
    "This schema definition should also be passed while creating the instance of `SparkMLModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this json format was used to test how to get probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"type\": \"string\", \"name\": \"text\"}], \"output\": {\"type\": \"float\", \"name\": \"probability\", \"struct\": \"vector\"}}\n"
     ]
    }
   ],
   "source": [
    "#this schema worked\n",
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "        \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"probability\",\n",
    "            \"type\": \"float\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I really wanted to get this two fields but it is not possible. I keep it to the future so we can improve the model a lot adding neutral reviews. Check the Logistic regression results for confusion matrix where probability is specified for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"type\": \"string\", \"name\": \"text\"}], \"output\": [{\"type\": \"double\", \"name\": \"prediction\"}, {\"type\": \"float\", \"name\": \"probability\", \"struct\": \"vector\"}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "        \n",
    "    ],\n",
    "    \"output\": \n",
    "        [{\n",
    "            \"name\": \"prediction\",\n",
    "            \"type\": \"double\"\n",
    "         }, \n",
    "          {\n",
    "            \"name\": \"probability\",\n",
    "            \"type\": \"float\",\n",
    "            \"struct\": \"vector\"\n",
    "          } \n",
    "          ]\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so only prediction will be taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"type\": \"string\", \"name\": \"text\"}], \"output\": {\"type\": \"double\", \"name\": \"prediction\"}}\n"
     ]
    }
   ],
   "source": [
    "#this schema worked\n",
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "        \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"prediction\",\n",
    "            \"type\": \"double\"\n",
    "         }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.sparkml.model.SparkMLPredictor at 0x7f06c1a30350>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 location of where you uploaded your trained and serialized SparkML model\n",
    "sparkml_data = 's3://{}/{}/{}'.format('sagemaker-us-east-2-446439287457', 'emr/sentiment/lr/mleap', 'model.tar.gz')\n",
    "model_name = 'sparkml-abalone-' + timestamp_prefix\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, \n",
    "                             role=role, \n",
    "                             sagemaker_session=sess, \n",
    "                             name=model_name,\n",
    "                             # passing the schema defined above by using an environment \n",
    "                             #variable that sagemaker-sparkml-serving understands\n",
    "                             env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json, \n",
    "                                  'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': \"application/jsonlines;data=text\"})\n",
    "\n",
    "\n",
    "endpoint_name = 'sparkml-abalone-ep-' + timestamp_prefix\n",
    "sparkml_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking the newly created inference endpoint with a payload to transform the data\n",
    "Now we will invoke the endpoint with a valid payload that `sagemaker-sparkml-serving` can recognize. There are three ways in which input payload can be passed to the request:\n",
    "\n",
    "* Pass it as a valid CSV string. In this case, the schema passed via the environment variable will be used to determine the schema. For CSV format, every column in the input has to be a basic datatype (e.g. int, double, string) and it can not be a Spark `Array` or `Vector`.\n",
    "\n",
    "* Pass it as a valid JSON string. In this case as well, the schema passed via the environment variable will be used to infer the schema. With JSON format, every column in the input can be a basic datatype or a Spark `Vector` or `Array` provided that the corresponding entry in the schema mentions the correct value.\n",
    "\n",
    "* Pass the request in JSON format along with the schema and the data. In this case, the schema passed in the payload will take precedence over the one passed via the environment variable (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing the payload in CSV format\n",
    "We will first see how the payload can be passed to the endpoint in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"features\":[0.5370864647910429,0.46291353520895695]}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "payload = \"I don't love Tidal :-)\"\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept='application/jsonlines')\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getting prediction instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "payload = \"I don't love it\"\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept='application/jsonlines')\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "payload = \"I love Tidal :-)\"\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept='application/jsonlines')\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing the payload in JSON format\n",
    "We will now pass a different payload in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4491398963389081,0.550860103661092\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.content_types import CONTENT_TYPE_NPY\n",
    "payload = {\"data\": [\"Tidal is amazing kkkk\"]}\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.content_types import CONTENT_TYPE_NPY\n",
    "payload = {\"data\": [\"Tidal is cool kkkk\"]}\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing the payload with both schema and the data\n",
    "Next we will pass the input payload comprising of both the schema and the data. \n",
    "Here i will create two payloads, with one I get the probability and with the another one I get the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4390124579424653,0.5609875420575346\n"
     ]
    }
   ],
   "source": [
    "payload_prob = {\n",
    "    \"schema\": {\n",
    "        \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"probability\",\n",
    "            \"type\": \"float\",\n",
    "            \"struct\": \"vector\"\n",
    "          }\n",
    "    },\n",
    "    \"data\": [\"tidal is not sad\"]\n",
    "}\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "payload_pred = {\n",
    "    \"schema\": {\n",
    "        \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"prediction\",\n",
    "            \"type\": \"double\"\n",
    "         }\n",
    "    },\n",
    "    \"data\": [\"I don't love it \"]\n",
    "}\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4491398963389081,0.550860103661092\n"
     ]
    }
   ],
   "source": [
    "payload_prob = {\n",
    "    \"schema\": {\n",
    "        \"input\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"probability\",\n",
    "            \"type\": \"float\",\n",
    "            \"struct\": \"vector\"\n",
    "          }\n",
    "    },\n",
    "    \"data\": [\"I am mad\"]\n",
    "}\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sess, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting the Endpoint (Optional)\n",
    "Next we will delete the endpoint so that you do not incur the cost of keeping it running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = sess.boto_session\n",
    "sm_client = boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not currently using this transformer because I had problems with wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " def get_stop_words_list():\n",
    "            stop_words_list = ['link','google','facebook','yahoo','rt','i', 'me', 'my', 'myself', 'tag'\n",
    "                              'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
    "                              \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his',\n",
    "                              'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    "                              'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "                              'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    "                              'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    "                              'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because',\n",
    "                              'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                              'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "                              'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                              'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "                              'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                              'only', 'own', 'same', 'so', 'than', 'too', 's', 't', 'can', 'will',\n",
    "                              'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've',\n",
    "                              'y', 'ain', 'ma', 'u', 'aren', 'ø', 'å', 'æ', 'b', 'c', 'd', 'e']\n",
    "\n",
    "            return stop_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important notes\n",
    "Since the notebook is not running locally you can not import the python classes as we did in the Data preparation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here you can check the files presents in your dir\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can acces the files via local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the question is how to send this modules to spark?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python2",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
